---
title: "HW4"
author: "Jieqi Tu"
date: "11/8/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Question 1
Download "nerve frings" data from http://www:stat:cmu:edu/~larry/all-of-ï¿½statistics/index:html.
Construct a 95% normal, pivotal and percentile confidence intervals for estimating the skewness and the median of the nerve data by bootstrapping.
```{r import data question1}
# Import dataset
nerve = read.table("nerve.dat", fill = T)
nerve = unlist(nerve) %>% na.omit() %>% as.numeric()
# Examine the sample size
sample_size = length(nerve); sample_size
```

We have 799 observed data in nerve dataset.

```{r check skewness}
# Define the function of skewness
skew.function = function(x) {
  mean((x - mean(x))^3)/mean((x - mean(x))^2)^(3/2)
}

# Calculate the skewness point estimate for nerve
nerve.skew = skew.function(nerve); nerve.skew

# Check the median of nerve
nerve.median = median(nerve); nerve.median

# Bootstrap
set.seed(1029)
n.boot = 1000
skewness = numeric(n.boot)
medians = numeric(n.boot)
for (i in 1:n.boot) {
  sample.data = sample(nerve, size = sample_size, replace = T)
  skewness[i] = skew.function(sample.data)
  medians[i] = median(sample.data)
}
par(mfrow = c(1, 2), pty = "s")
plot(density(skewness), main = "density of skewness")
plot(density(medians), main = "density of medians")
```

We can see that the skewness is relatively bell shaped, but medians have three peaks in the plot of density. Now we want to contruct the 95% confidence intervals for them.


```{r CI calculation}
# Calculate CI for skewness
skewness.ci.normal = c(
  low = nerve.skew + qnorm(0.025)*sd(skewness),
  high = nerve.skew + qnorm(0.975)*sd(skewness),
  length = qnorm(0.975)*sd(skewness) - qnorm(0.025)*sd(skewness)
)

skewness.ci.pivot = c(
  low = 2*nerve.skew - quantile(skewness, 0.975),
  high = 2*nerve.skew - quantile(skewness, 0.025),
  length = -quantile(skewness, 0.025) + quantile(skewness, 0.975)
)

skewness.ci.percentile = c(
  low = quantile(skewness, 0.025),
  high = quantile(skewness, 0.975),
  length = quantile(skewness, 0.975) - quantile(skewness, 0.025)
)
rbind(skewness.ci.normal, skewness.ci.pivot, skewness.ci.percentile) %>% as.data.frame() %>% knitr::kable()

# Calculate CI for medians
median.ci.normal = c(
  low = nerve.median + qnorm(0.025)*sd(medians),
  high = nerve.median + qnorm(0.975)*sd(medians),
  length = qnorm(0.975)*sd(medians) - qnorm(0.025)*sd(medians)
)
median.ci.pivot = c(
  low = 2*nerve.median - quantile(medians, 0.975),
  high = 2*nerve.median - quantile(medians, 0.025),
  length = -quantile(medians, 0.025) + quantile(medians, 0.975)
)

median.ci.percentile = c(
  low = quantile(medians, 0.025),
  high = quantile(medians, 0.975),
  length = quantile(medians, 0.975) - quantile(medians, 0.025)
)
rbind(median.ci.normal, median.ci.pivot, median.ci.percentile) %>% as.data.frame() %>% knitr::kable()

```

## Question 2
```{r question 2}
# Import data
treatments = data.frame(placebo = c( 9243,  9671, 11792, 13357,  9055,  6290, 12412, 18806),
                            old = c(17649, 12013, 19979, 21816, 13850,  9806, 17208, 29044),
                            new = c(16449, 14614, 17274, 23798, 12560, 10157, 16570, 26325))

treatments$`old-placebo` = treatments$old - treatments$placebo
treatments$`new-old` = treatments$new - treatments$old
treatments %>% knitr::kable()

sample.size = nrow(treatments)

# Define function of bioequivalence
theta = function(y, z) {
  mean(y)/mean(z)
}
theta_abs = function(y, z) {
  abs(mean(y)/mean(z))
}
z = treatments$`old-placebo`
y = treatments$`new-old`

theta.estimate = theta_abs(y, z); theta.estimate
```

Construct 95% CI for bioequivalence using bootstrap.
```{r ci calculation for theta}
# Bootstrap
set.seed(1029)
n.boot = 1000
theta_boot = numeric(n.boot)
theta_abs_boot = numeric(n.boot)
for (i in 1:n.boot) {
  row_n = sample(1:sample.size, size = sample.size, replace = T)
  theta_boot[i] = theta(treatments$`new-old`[row_n], treatments$`old-placebo`[row_n])
  theta_abs_boot[i] = theta_abs(treatments$`new-old`[row_n], treatments$`old-placebo`[row_n])
}

# Bootstrap results for absolute value of theta
summary(unlist(theta_abs_boot))
sd(unlist(theta_abs_boot))

# Bootstrap results for theta
summary(unlist(theta_boot))
sd(unlist(theta_boot))

# Construct 95% CI
theta_abs_boot = unlist(theta_abs_boot)
theta_boot = unlist(theta_boot)
# (1) Normal method
CI.theta.normal = c(
  lower = theta.estimate + qnorm(0.025)*sd(theta_boot), 
  upper = theta.estimate + qnorm(0.975)*sd(theta_boot),
  length_of_CI = qnorm(0.975)*sd(theta_boot)-qnorm(0.025)*sd(theta_boot)
)

# (2) Pivotal method
CI.theta.pivot = c(
  lower = 2*theta.estimate - quantile(theta_boot, 0.975), 
  upper = 2*theta.estimate - quantile(theta_boot, 0.025),
  length_of_CI = - quantile(theta_boot, 0.025) + quantile(theta_boot, 0.975)
)

# (3) Percentile method
CI.theta.pencentile = c(
  lower = quantile(theta_abs_boot, 0.025), 
  upper = quantile(theta_abs_boot, 0.975),
  length_of_CI = quantile(theta_abs_boot, 0.975)- quantile(theta_abs_boot, 0.025)
)

# Check the results
rbind(CI.theta.normal, CI.theta.pivot, CI.theta.pencentile)
```

## Question 3
```{r question 3}
# Create data from Normal distribution with mean 5 and sd 1
set.seed(20201111)
x = rnorm(100, mean = 5, sd = 1)
n = length(x)

# Define the function for theta
theta = function(x) {exp(mean(x))}

# Point estimation
theta.hat = theta(x); theta.hat

# Bootstrap
n.sim = 1000
theta.boot = xbar.boot = numeric(n.sim)
for (i in 1:n.sim) {
  x.sample = sample(x, size = n, replace = T)
  theta.boot[i] = theta(x.sample)
  xbar.boot[i] = mean(x.sample)
}

# (1) normal method
CI.normal.theta = c(
  lower = theta.hat + qnorm(0.025)*sd(theta.boot), 
  upper = theta.hat + qnorm(0.975)*sd(theta.boot),
  length_of_CI = qnorm(0.975)*sd(theta.boot)-qnorm(0.025)*sd(theta.boot)
)

# (2) pivotal method
CI.pivot.thata = c(
  lower = 2*theta.hat - quantile(theta.boot, 0.975), 
  upper = 2*theta.hat - quantile(theta.boot, 0.025),
  length_of_CI = - quantile(theta.boot, 0.025) + quantile(theta.boot, 0.975)
)

# (3) percentile method
CI.percent.theta = c(
  lower = quantile(theta.boot, 0.025), 
  upper = quantile(theta.boot, 0.975),
  length_of_CI = quantile(theta.boot, 0.975)- quantile(theta.boot, 0.025)
)

# Check the results
rbind(CI.normal.theta, CI.pivot.thata, CI.percent.theta)

theta.boot = theta.boot %>% as.data.frame()
colnames(theta.boot) = c("theta.boot")

# Generate true sampling distribution
theta.true = numeric(n.sim)
for (i in 1:n.sim) {
  true.sample = rnorm(100, 5, 1)
  theta.true[i] = theta(true.sample)
}

theta.true = theta.true %>% as.data.frame()
colnames(theta.true) = c("theta.true")

library(ggpubr)
boot.plot = 
theta.boot %>% 
  ggplot(aes(x = theta.boot)) + geom_histogram() + theme_bw() + geom_vline(xintercept = exp(5), linetype = "dashed", color = "blue")


true.plot = theta.true %>% 
  ggplot(aes(x = theta.true)) + geom_histogram() + theme_bw() + geom_vline(xintercept = exp(5), linetype = "dashed", color = "blue")

ggarrange(boot.plot, true.plot,
          labels = c("Bootstrap", "True Sampling"))
```

From the histograms, we could see that the distribution of both true sampling and boostrap have the similar mean (close to $e^5$, represented by the blue dashed lines). The distributions look very close to each other. This indicates that the bootstrap distribution covers the majority of true sampling distribution, and it performs very well.

## Question 4
```{r question 4}
# Generate the sample data
set.seed(1029)
theta = 1
x = runif(50, 0, theta)
n = length(x)

# point estimate
theta.max = max(x); theta.max

# bootstrap
n.sim = 1000
max.theta = numeric(length = n.sim)
for (i in 1:n.sim) {
  x.new = sample(x, size = n, replace = T)
  max.theta[i] = max(x.new)
}

# 95% bootstrap confidence interval for theta hat
# (1) normal method
CI.normal.max = c(
  lower = theta.max + qnorm(0.025)*sd(max.theta),
  upper = theta.max + qnorm(0.975)*sd(max.theta),
  length_of_CI = qnorm(0.975)*sd(max.theta)-qnorm(0.025)*sd(max.theta)
)

# (2) pivital method
CI.pivot.max = c(
  lower = 2*theta.max - quantile(max.theta, 0.975), 
  upper = 2*theta.max - quantile(max.theta, 0.025),
  length_of_CI = - quantile(max.theta, 0.025) + quantile(max.theta, 0.975)
)

# (3) percentile method
CI.percent.max = c(
  lower = quantile(max.theta, 0.025), 
  upper = quantile(max.theta, 0.975),
  length_of_CI = quantile(max.theta, 0.975)- quantile(max.theta, 0.025)
)

# Check results
rbind(CI.normal.max, CI.pivot.max, CI.percent.max)
```

We now want to check the distribution of $\hat\theta$.
```{r question4 distribution}
# bootstrap distribution of thetahat
theta.boot = max.theta %>% as.data.frame()
colnames(theta.boot) = c("theta.hat")

distribution.boot = 
  theta.boot %>% 
  ggplot(aes(x = theta.hat)) + geom_density() + theme_bw()

# generate true sampling distribution
theta.true = numeric(n.sim)
set.seed(1029)
for (i in 1:n.sim) {
  newsample = runif(50, 0, theta)
  theta.true[i] = max(newsample)
}

theta.true = theta.true %>% as.data.frame()
colnames(theta.true) = c("theta.hat")

distribution.true = 
  theta.true %>% 
  ggplot(aes(x = theta.hat)) + geom_density() + theme_bw()

ggarrange(distribution.true,
          distribution.boot,
          labels = c("True Sampling",
                     "Bootstrap Distribution"))
```

From the plots, we could see that bootstrap resampling failed to capture the distribution of true sampling.
It is because the true sampling distribution has an extremely heavy tail.
Heavy tail distributions often cause problems for asymptotic and bootstrap inference. 
Therefore, we cannot use bootstrap method when the resampling distribution has extremely heavy tails.
However, according to Arcones and Gine (1989), we can solve this problem by using m out n bootstrap, which is performed below. It also works well when we sample without replacement.
```{r q4 m out of n}
# define function
m_outof_n_bootstrap = function(n=50, m=10, replace = T){
set.seed(1029) 
theta = 1
x = runif(n, 0, theta)
# Bootstrap (max)
B = 1000
B.max = numeric(length = B) 

for (i in 1:B) {
x.new = sample(x, size = m, replace = replace)
B.max[i] = max(x.new)}
# percentile method
CI.percent.max = c(lower = quantile(B.max, 0.025), upper = quantile(B.max, 0.975),
length_of_CI = quantile(B.max, 0.975)- quantile(B.max, 0.025))
return(CI.percent.max)
}

m_outof_n_bootstrap(n = 50, m = 10, replace = T)
m_outof_n_bootstrap(n = 50, m = 10, replace = F)
m_outof_n_bootstrap(n = 500, m = 10, replace = T)
m_outof_n_bootstrap(n = 500, m = 10, replace = F)
```

## Question 5
```{r question 5}
# Import data
scores = data.frame( GPA = c(3.39, 3.30, 2.81, 3.03, 
                             3.44, 3.07, 3.00, 3.43, 
                             3.36, 3.13, 3.12, 2.74,
                             2.76, 2.88, 3.96), 
                     LSAT = c(576, 635, 558, 578, 
                              666, 580, 555, 661, 
                              651, 605, 653, 575, 
                              545, 572, 594))

# Sample size
n = nrow(scores)

# point estimate
cor.hat = cor(scores$GPA, scores$LSAT)
cor.hat

# Bootstrap
set.seed(1029)
n.sim = c(100, 1000, 10000)
cor.boot_100 = numeric(n.sim[1])
cor.boot_1000 = numeric(n.sim[2])
cor.boot_10000 = numeric(n.sim[3])

for (i in 1:n.sim[1]) {
  idx = sample(1:n, size = n, replace = T)
  cor.boot_100[i] = cor(scores$GPA[idx], scores$LSAT[idx])
}

for (i in 1:n.sim[2]) {
  idx = sample(1:n, size = n, replace = T)
  cor.boot_1000[i] = cor(scores$GPA[idx], scores$LSAT[idx])
}

for (i in 1:n.sim[3]) {
  idx = sample(1:n, size = n, replace = T)
  cor.boot_10000[i] = cor(scores$GPA[idx], scores$LSAT[idx])
}

plot(density(cor.boot_100))
plot(density(cor.boot_1000))
plot(density(cor.boot_10000))
```

Three bootstrap resampling distributions are similar. Then we want to check the confidence interval.
```{r q5 CI}
# (1) normal method
CI.normal.cor = data.frame(n.sim = n.sim,
                           lower = sapply(n.sim, function(b) cor.hat + qnorm(0.025)*sd(cor.boot_10000[1:b])), 
                           upper = sapply(n.sim, function(b) cor.hat + qnorm(0.975)*sd(cor.boot_10000[1:b])))

# (2) pivot method
CI.pivot.cor = data.frame(n.sim = n.sim,
                          lower = sapply(n.sim, function(b) 2*cor.hat - quantile(cor.boot_10000[1:b], 0.975)), 
                          upper = sapply(n.sim, function(b) 2*cor.hat - quantile(cor.boot_10000[1:b], 0.025)))

# (3) pencentile method
CI.percent.cor = data.frame(n.sim = n.sim,
                            lower = sapply(n.sim, function(b) quantile(cor.boot_10000[1:b], 0.025)), 
                            upper = sapply(n.sim, function(b) quantile(cor.boot_10000[1:b], 0.975)))
```

From the results we could see that, small sample size leads to broad confidence interval. However, we want to narrow down the confidence interval. 
We found that when the bootstrap iteration times increase, most 95% confidence intervals get narrower.
But still, the interval is still very wide. Since the GPA and LSAT scores are not highly correlated, we might conclude that we cannot consider a student just by GPA or LSAT. We need to combine the two scores to have a more comprehensive view for one student.